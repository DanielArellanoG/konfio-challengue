Planteamiento
# Stack de referencia
RegiÃ³n principal: us-east-1.

La API de originaciÃ³n de crÃ©dito con la siguiente arquitectura:
Amazon API Gateway o ALB exponiendo la API.
Backend en Amazon ECS (Fargate).
Base de datos Amazon Aurora PostgreSQL.
Dependencias:
Proveedor externo de scoring.
Servicio interno de autenticaciÃ³n.

Logs en Amazon CloudWatch Logs.
MÃ©tricas en Amazon CloudWatch.
Opcional: AWS X-Ray y/o APM externo.

Infraestructura gestionada con Terraform en cuentas separadas por entorno.



# 1. Confiabilidad de un servicio crÃ­tico
ğŸ¯ Contexto
TrabajÃ¡s como SRE en una fintech. Uno de los servicios mÃ¡s crÃ­ticos es la API de
originaciÃ³n de crÃ©ditos:
Capa de entrada: Amazon API Gateway o Application Load Balancer .
CÃ³mputo: Backend en Amazon ECS (Fargate)
Datos: Amazon Aurora PostgreSQL .
Dependencias:
Proveedor externo de scoring.
Servicio interno de autenticaciÃ³n.
Observabilidad: Logs y mÃ©tricas bÃ¡sicas.
Esta API es crÃ­tica: cualquier degradaciÃ³n afecta directamente la originaciÃ³n de
crÃ©ditos.
Actualmente no hay estrategia formal de confiabilidad ni observabilidad
estructurada.

1. Estrategia de Confiabilidad
DiseÃ±Ã¡ una estrategia integral de confiabilidad para este servicio:
MÃ©tricas de confiabilidad: Â¿QuÃ© indicadores definirÃ­as para medir la salud del
servicio? Â¿CÃ³mo los calcularÃ­as?
Objetivos de servicio: Â¿QuÃ© compromisos de calidad establecerÃ­as con el
negocio? Â¿CÃ³mo los justificarÃ­as?
Fuentes de datos: Â¿QuÃ© servicios AWS y herramientas usarÃ­as para obtener
estas mÃ©tricas?
2. Arquitectura de Observabilidad
DescribÃ­ cÃ³mo implementarÃ­as observabilidad completa:
MÃ©tricas: Â¿QuÃ© mÃ©tricas capturarÃ­as y dÃ³nde las almacenarÃ­as?
Logs: Â¿CÃ³mo estructurarÃ­as el logging? Â¿QuÃ© informaciÃ³n incluirÃ­as?
Trazas: Â¿CÃ³mo implementarÃ­as tracing distribuido?
Dashboards: Â¿QuÃ© visualizaciones crearÃ­as y para quÃ© audiencias?
3. Sistema de Alertas
DiseÃ±Ã¡ un sistema de alertas efectivo:
DetecciÃ³n temprana: Â¿QuÃ© seÃ±ales usarÃ­as para detectar degradaciÃ³n antes
de que impacte usuarios?
Incidentes crÃ­ticos: Â¿QuÃ© condiciones activarÃ­an alertas de alta prioridad?
GestiÃ³n de ruido: Â¿CÃ³mo evitarÃ­as fatiga de alertas?
Escalamiento: Â¿CÃ³mo estructurarÃ­as las notificaciones por severidad?
4. GestiÃ³n Operativa
ExplicÃ¡ cÃ³mo gestionarÃ­as la confiabilidad en producciÃ³n:
Monitoreo continuo: Â¿CÃ³mo harÃ­as seguimiento del cumplimiento de
objetivos?
Revisiones periÃ³dicas: Â¿QuÃ© procesos establecerÃ­as para evaluar y ajustar la
estrategia?
ComunicaciÃ³n con negocio: Â¿CÃ³mo reportarÃ­as el estado de confiabilidad a
stakeholders no tÃ©cnicos?

# 2. AnÃ¡lisis de Incidente & Postmortem
ğŸ¯ Contexto
Incidente reportado:
"Los clientes estÃ¡n tardando mucho en completar solicitudes de crÃ©dito y
algunos no logran terminar el proceso".
LÃ­nea de tiempo:
09:00 â€“ Negocio reporta caÃ­da del 40% en finalizaciÃ³n de flujos de crÃ©dito.
09:10 â€“ Dashboard de producto muestra reducciÃ³n en crÃ©ditos procesados.
09:15 â€“ Observabilidad muestra:
Aumento significativo de latencia en /loan/apply .
Errores intermitentes hacia proveedor externo de scoring.
09:25 â€“ Logs muestran:
Timeouts hacia proveedor externo.
Errores de conexiÃ³n hacia Aurora.
09:40 â€“ Se identifica que durante la noche se desplegÃ³ nueva versiÃ³n del
servicio de scoring con cambios en polÃ­tica de reintentos.
10:00 â€“ MitigaciÃ³n:
Rollback del servicio de scoring.
Ajuste de configuraciÃ³n de reintentos.
NormalizaciÃ³n del servicio.

1. Respuesta a Incidentes
DescribÃ­ tu aproximaciÃ³n durante los primeros 60 minutos:
InvestigaciÃ³n inicial: Â¿QuÃ© datos revisarÃ­as primero y en quÃ© orden?
FormulaciÃ³n de hipÃ³tesis: Â¿QuÃ© posibles causas considerarÃ­as?
Estrategia de mitigaciÃ³n: Â¿QuÃ© acciones priorizarÃ­as para reducir impacto?
ComunicaciÃ³n: Â¿CÃ³mo mantendrÃ­as informados a los stakeholders?
1. AnÃ¡lisis de Causa RaÃ­z
DesarrollÃ¡ un anÃ¡lisis profundo:
Causa raÃ­z principal: Identifica y explica la causa fundamental.
Factores contribuyentes: Â¿QuÃ© elementos del sistema permitieron que este
problema ocurriera?
Puntos de falla: Â¿DÃ³nde fallaron las defensas del sistema?
3. Postmortem
RedactÃ¡ un postmortem completo que incluya:
Resumen ejecutivo orientado a negocio.
Impacto detallado (usuarios, duraciÃ³n, alcance).
CronologÃ­a completa de eventos.
AnÃ¡lisis tÃ©cnico de la causa raÃ­z.
Acciones correctivas inmediatas.
Plan de mejoras a mediano y largo plazo.
4. PrevenciÃ³n y Mejoras
ProponÃ© mejoras sistÃ©micas:
Observabilidad: Â¿QuÃ© gaps de visibilidad identificaste?
Arquitectura: Â¿QuÃ© cambios arquitectÃ³nicos prevendrÃ­an recurrencias?
Procesos: Â¿QuÃ© mejoras en procesos de desarrollo y despliegue
implementarÃ­as?
Cultura: Â¿CÃ³mo fortalecerÃ­as la cultura de confiabilidad en el equipo?


# 3. IaC
1. OpciÃ³n A â€“ MÃ³dulo Terraform de Observabilidad
DiseÃ±Ã¡ un mÃ³dulo Terraform reutilizable para estandarizar observabilidad de APIs
en AWS.
Requisitos:
Parametrizable para diferentes tipos de APIs (ALB/API Gateway).
CreaciÃ³n automÃ¡tica de mÃ©tricas, alarmas y dashboards.
ConfiguraciÃ³n flexible de umbrales y notificaciones.
DocumentaciÃ³n de uso y ejemplos.
Entregable:
CÃ³digo Terraform con estructura modular.
DocumentaciÃ³n de variables y outputs.
Ejemplo de uso del mÃ³dulo.
2. OpciÃ³n B â€“ Herramienta de AnÃ¡lisis de Logs
DesarrollÃ¡ una herramienta para anÃ¡lisis automatizado de logs de CloudWatch.
Requisitos:
Procesamiento de logs exportados de CloudWatch.
CÃ¡lculo de mÃ©tricas de confiabilidad.
DetecciÃ³n automÃ¡tica de anomalÃ­as.
Reportes estructurados con recomendaciones.
Entregable:
Script en Python/Go con funcionalidad completa.
DocumentaciÃ³n de uso y configuraciÃ³n.
Ejemplos de anÃ¡lisis y outputs.


